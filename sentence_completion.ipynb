{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ium_lnH8M8oH",
        "outputId": "6384912f-cd22-490f-9014-41cb007399fb"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo9e7z4fM9jl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSyOmD_7M9hz",
        "outputId": "a6f32df8-7b5f-4b3c-e2e1-5588be318cf6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIqJZL5OM9fW",
        "outputId": "663bcc09-29cc-4d4a-8beb-a1807e76ac4c"
      },
      "outputs": [],
      "source": [
        "model_name = \"openai-community/gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5weJJgbOROS"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pol6ZUqyOVJt",
        "outputId": "74811b7b-dc4a-4724-ca41-46f07079dfdc"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
        "print(f\"Dataset loaded with {len(dataset['train'])} training examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJA37xzlM9cz"
      },
      "outputs": [],
      "source": [
        "def prepare_training_example(example, max_length=512):\n",
        "    \"\"\"\n",
        "    Prepares a dataset example for instruction fine-tuning by tokenizing and\n",
        "    properly formatting inputs and labels.\n",
        "\n",
        "    Args:\n",
        "        example (dict): Dictionary containing 'instruction', 'input', and 'output' keys\n",
        "        max_length (int): Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        dict: Processed example with input_ids, attention_mask, and labels\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Format the prompt and full text\n",
        "        if example['input']:\n",
        "            prompt = f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nOutput:\"\n",
        "        else:\n",
        "            # Handle cases where input is empty\n",
        "            prompt = f\"Instruction: {example['instruction']}\\nOutput:\"\n",
        "\n",
        "        full_text = prompt + \" \" + example[\"output\"]\n",
        "\n",
        "        # Tokenize the full sequence\n",
        "        tokenized = tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Tokenize just the prompt to find its length\n",
        "        prompt_tokens = tokenizer(\n",
        "            prompt,\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        # Set up labels: -100 for prompt tokens (to be ignored by loss function)\n",
        "        labels = tokenized[\"input_ids\"].copy()\n",
        "        prompt_length = len(prompt_tokens[\"input_ids\"])\n",
        "\n",
        "        # Set prompt tokens to -100 so they're ignored in loss calculation\n",
        "        for i in range(prompt_length):\n",
        "            if i < len(labels):\n",
        "                labels[i] = -100\n",
        "\n",
        "        # Also mask padding tokens in the labels\n",
        "        for i in range(len(labels)):\n",
        "            if tokenized[\"attention_mask\"][i] == 0:  # This is a padding token\n",
        "                labels[i] = -100\n",
        "\n",
        "        tokenized[\"labels\"] = labels\n",
        "        return tokenized\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example: {e}\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": [tokenizer.pad_token_id] * 2,\n",
        "            \"attention_mask\": [0] * 2,\n",
        "            \"labels\": [-100] * 2\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a98995ee357a466e9fcdeb32423ce216",
            "275841376d12499fa412cd489c6a1d9e",
            "5c56e138efbf4b9aa398ab0149713a6e",
            "bef29f18e88a4418a4c4097df923ab39",
            "4b438c4dd83b4d789a6c7000e335c863",
            "562acb7d4dba41e687ca99f5e7fd5e9c",
            "4f32f766658b424a85a399fef65d0a89",
            "a85301c355c944369f6cce39b68c4816",
            "72f7cb3ba18b48ffb47a45350d1f2168",
            "cdfa33c52e674cda8d3aa0f262b96874",
            "b3a774ef13f84e1ab7870fafe826eef9"
          ]
        },
        "id": "RLUkB4T_M9Z_",
        "outputId": "aa04f169-1219-4732-8285-64ba0e575503"
      },
      "outputs": [],
      "source": [
        "max_length = 512  # Adjust based on your needs and GPU memory\n",
        "processed_dataset = dataset.map(\n",
        "    lambda x: prepare_training_example(x, max_length=max_length),\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    desc=\"Processing dataset\",\n",
        "    num_proc=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5nQq0K9PVof"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, test_examples, device=\"cuda\", max_new_tokens=100):\n",
        "    \"\"\"\n",
        "    Evaluates a model on a list of test examples.\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate\n",
        "        tokenizer: The tokenizer to use\n",
        "        test_examples: List of dictionaries with 'instruction', 'input', and 'reference_output' keys\n",
        "        device: Device to run inference on\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with generated responses and metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    for example in test_examples:\n",
        "        instruction = example['instruction']\n",
        "        input_text = example.get('input', '')\n",
        "        reference = example.get('reference_output', '')\n",
        "\n",
        "        # Format the prompt based on whether input is provided\n",
        "        if input_text:\n",
        "            prompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n",
        "        else:\n",
        "            prompt = f\"Instruction: {instruction}\\nOutput:\"\n",
        "\n",
        "        # Tokenize and move to device\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode and extract only the response part\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = full_output[len(prompt):].strip()\n",
        "\n",
        "        results.append({\n",
        "            'instruction': instruction,\n",
        "            'input': input_text,\n",
        "            'generated_output': response,\n",
        "            'reference_output': reference\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4ASWsI8Pm0S"
      },
      "outputs": [],
      "source": [
        "# Sample test examples\n",
        "test_examples = [\n",
        "    {\n",
        "        'instruction': 'Write a short poem',\n",
        "        'input': 'about artificial intelligence',\n",
        "        'reference_output': 'In silicon dreams and neural might,\\nA new mind awakens to the light.\\nNot born of flesh but human thought,\\nIntelligence that we have wrought.'\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Translate the following sentence to German\",\n",
        "        \"input\": \"I am hungry\",\n",
        "        \"reference_output\": \"Ich habe Hunger\"\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Summarize the main idea',\n",
        "        'input': 'The Internet of Things (IoT) refers to the billions of physical devices around the world that are now connected to the internet, collecting and sharing data.',\n",
        "        'reference_output': 'The Internet of Things (IoT) is a network of billions of internet-connected physical devices worldwide that collect and share data.'\n",
        "    },\n",
        "    {\n",
        "        'instruction': 'Explain the concept of photosynthesis in simple terms',\n",
        "        'input': '',\n",
        "        'reference_output': 'Photosynthesis is how plants make their own food. They use sunlight, water, and carbon dioxide to create energy and oxygen. It\\'s like plants cooking their meals using sunlight as the heat source.'\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPiFfLzVPrpV"
      },
      "outputs": [],
      "source": [
        "# Function to run tests and display results\n",
        "def run_model_evaluation(model_name=\"Base Model\", save_results=False):\n",
        "    print(f\"\\n===== {model_name} Evaluation =====\")\n",
        "\n",
        "    model.to(device)\n",
        "    results = evaluate_model(model, tokenizer, test_examples, device)\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Instruction: {result['instruction']}\")\n",
        "        if result['input']:\n",
        "            print(f\"Input: {result['input']}\")\n",
        "        print(f\"\\nGenerated output: {result['generated_output']}\")\n",
        "        if result['reference_output']:\n",
        "            print(f\"Reference output: {result['reference_output']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    if save_results:\n",
        "        import json\n",
        "        import os\n",
        "        results_dir = \"./evaluation_results\"\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        with open(f\"{results_dir}/{model_name.replace(' ', '_').lower()}_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results saved to {results_dir}/{model_name.replace(' ', '_').lower()}_results.json\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7RlUSHUWYdO",
        "outputId": "ee199619-24a8-4244-d64f-a32c9ce4742f"
      },
      "outputs": [],
      "source": [
        "# 1. Test before fine-tuning\n",
        "print(\"\\n\\n=============== BEFORE FINE-TUNING EVALUATION ===============\")\n",
        "before_results = run_model_evaluation(\"Before Fine-tuning\", save_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-WW9H9fQQcQ"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    inference_mode=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgxUM34PQY4_",
        "outputId": "ab5edf6a-84c8-4526-bd7b-79fca66939f0"
      },
      "outputs": [],
      "source": [
        "# Prepare model for LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV8eYwqaQfCQ"
      },
      "outputs": [],
      "source": [
        "# Set up training arguments\n",
        "output_dir = \"./gpt2-alpaca-lora\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    max_steps=1000,  # Adjust based on dataset size and needs\n",
        "    learning_rate=2e-4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"tensorboard\",\n",
        "    run_name=\"gpt2-alpaca-lora\",\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syyd6a1DQmz2"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Not using masked language modeling\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iwUHPnaRQqbK",
        "outputId": "414ec67d-fc9c-45bb-a04a-c018cb35cbf9"
      },
      "outputs": [],
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwiD5dOtSauv",
        "outputId": "bba20195-dbd4-4698-ca19-f72aa99167d1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\n=============== AFTER FINE-TUNING EVALUATION ===============\")\n",
        "after_results = run_model_evaluation(\"After Fine-tuning\", save_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCWjUZaCSace"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "275841376d12499fa412cd489c6a1d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562acb7d4dba41e687ca99f5e7fd5e9c",
            "placeholder": "​",
            "style": "IPY_MODEL_4f32f766658b424a85a399fef65d0a89",
            "value": "Processing dataset (num_proc=4): 100%"
          }
        },
        "4b438c4dd83b4d789a6c7000e335c863": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f32f766658b424a85a399fef65d0a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562acb7d4dba41e687ca99f5e7fd5e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c56e138efbf4b9aa398ab0149713a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85301c355c944369f6cce39b68c4816",
            "max": 52002,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72f7cb3ba18b48ffb47a45350d1f2168",
            "value": 52002
          }
        },
        "72f7cb3ba18b48ffb47a45350d1f2168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a85301c355c944369f6cce39b68c4816": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98995ee357a466e9fcdeb32423ce216": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_275841376d12499fa412cd489c6a1d9e",
              "IPY_MODEL_5c56e138efbf4b9aa398ab0149713a6e",
              "IPY_MODEL_bef29f18e88a4418a4c4097df923ab39"
            ],
            "layout": "IPY_MODEL_4b438c4dd83b4d789a6c7000e335c863"
          }
        },
        "b3a774ef13f84e1ab7870fafe826eef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bef29f18e88a4418a4c4097df923ab39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdfa33c52e674cda8d3aa0f262b96874",
            "placeholder": "​",
            "style": "IPY_MODEL_b3a774ef13f84e1ab7870fafe826eef9",
            "value": " 52002/52002 [01:14&lt;00:00, 436.07 examples/s]"
          }
        },
        "cdfa33c52e674cda8d3aa0f262b96874": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
